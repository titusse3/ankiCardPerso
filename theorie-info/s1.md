# s1

## Discrète

Quand dis t'on qu'une source d'évènement est discrèt ?

%

Lorsque que sont alphabet(= le nombre d'évènement possible) est finie.

## Sans mémoire

Expliquer ce que signifie pour une source d'évènement d'être sans mémoire ?

%

Si on note \\(S_i\\) la sortie de la source à l’instant \\(i\\), dire que la source
\\(S\\) est sans mémoire signifie que les variables aléatoires \\(S0, S1, ...\\) sont indépendantes.

## Notation

Comment note t'on la probabilité qu'une source émette le symbole \\(a\\) ?

%

\\( \mathbb{P}\{S = a\} \\)

## Information

Donner la formule de quantification de l'information pour une probabilité \\(p\\).

%

On note \\(I\\) la quantité d'information, \\(I = -log p\\).

## Entropie

Donner la formule de l'antropie pour une source discète \\(S\\).

%

\\(H(S) = - \sum_{a \in \mathcal{A}} \mathbb{P} \{S = a\} log_2 \mathbb{P} \{S = a\} \\)

## Entropie uniter

Qu'elle est l'unité de l'entropie ?

%

C'est le bit.

## Entropie 

Donner les deux intéprétation possible pour l'entropie ?

%

- La quantité de bit nécessaire pour lever l'incertitude
- Une mesure de l'incertitiude

## Conjointe

Comment note t'on la probabilité conjointe de deux alphabets ?

%

\\(\forall (a, b) \in \mathcal{A}\times\mathcal{B}, \mathbb{P} \{(X, Y) = (a, b)\} = \mathbb{P}\{X=a, Y=b\}\\)

## Indépendance

Comment peut t'on écrire la conjointe de deux alphabets si \\(X\\) et \\(Y\\) sont indépendants ?

%

\\(\forall (a, b) \in \mathcal{A}\times\mathcal{B}, \mathbb{P} \{(X, Y) = (a, b)\} = \mathbb{P}\{X = a\} \mathbb{P}\{Y = b\}\\)

## Entropie conjointe

Donner la valeur de l'entropie conjointe.

%

\\(H(X, Y) = - \sum_{(a, b) \in A\times B} \mathbb{P} \{X = a, Y=b\} log_2 \mathbb{P}\{X = a, Y= b\}\\)

## Conjointe 1

Que vaut \\( \forall (a, a') \in \mathcal{A}\times \mathcal{A}, \mathbb{P}\{(X, X)\} \\)

%

\\(\forall (a, a') \in \mathcal{A}\times \mathcal{A}, \mathbb{P}\{(X, X) = (a, a')\} = \begin{align*}
\mathbb{P}\{X = a\} \; si \; a = a'\\
0 \; sinon
\end{align*}\\)

## Entropie conjointe 1

Que vaut \\(H(X, X)\\) ?

%

\\(H(X, X) = H(X)\\)

## Entropie conjonite 2

Si \\(X\\) et \\(Y\\) sont indépendant que vaut \\(H(X, Y)\\) ?

%

\\(H(X, Y) = H(X) + H(Y)\\)

## Conditionnelle

Donner la formule de \\(X\\) sachant \\(Y\\).

%

\\(\mathbb{P}\{X = a | Y = b\} = \dfrac{\mathbb{P} \{X = a, Y = b\}}{\mathbb{P} \{Y = b\}} \\)

## Entropie conditionnelle

Donner la formule de l'entropie conditionnelle.

%

\\(H(X|Y) = - \sum_{(a, b) \in \mathcal{A}\times \mathcal{B}} \mathbb{P} \{X = a, Y = b\} log_2 \mathbb{P}\{X = a | Y = b\}\\)

## Formule

Donner une formulle lians l'entropie conjointe a celle conditionnelle.

%

\\(H(X, Y) = H(X | Y) + H(Y)\\)

## Entropie conditionelle 

Donner la formule pour l'entropi conditionnel si \\(X\\) et \\(Y\\) sont indépendant.

%

\\(H(X | Y) = H(X)\\)

## Information mutuelle

Donner la formule de l'information mutuelle entre \\(X\\) et \\(Y\\).

%

\\(I(X;Y) = H(X) - H(X|Y)\\)

## Information mutuelle 1

Donner le résultat de \\(I(X;Y)\\) si \\(X\\) et \\(Y\\) indépendant.

%

\\(I(X;Y) = 0\\)

## Information mutuelle 2

Donner une formule de l'information mutuelle en fonction de l'entropie conjonctive.

%

\\(I(X; Y) = H(X) + H(Y) - H(X, Y)\\)

## Prop Information mutuelle

Donner le propriété sur le signe de l'information mutuelle.

%

\\(I(X; Y) \geq 0\\)


